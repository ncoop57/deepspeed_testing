You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `
loading configuration file https://huggingface.co/t5-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1adb1f57c3579debfcce7b94ee03f6144e0ff7a0c2825e48b3f9cde9ce290c7d.35a5d3297357a9ea0fccdf170df8d287f1cad2ee810bca042f98c531c0cab2c6
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 4096,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.6.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading configuration file https://huggingface.co/t5-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1adb1f57c3579debfcce7b94ee03f6144e0ff7a0c2825e48b3f9cde9ce290c7d.35a5d3297357a9ea0fccdf170df8d287f1cad2ee810bca042f98c531c0cab2c6
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 4096,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.6.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file https://huggingface.co/t5-large/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/71ee551f54e246045a7b94dd449c33759924b864712e6d235bbba5245c9f6296.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d
loading file https://huggingface.co/t5-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/276094e085ecb12227136f2e755dc1f68be6f5da32df55ebfb104c791fbbc3c1.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529
loading file https://huggingface.co/t5-large/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/t5-large/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/t5-large/resolve/main/tokenizer_config.json from cache at None
loading weights file https://huggingface.co/t5-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/750feca8cedcd171eb121bd47c3ae16924a473d89f334c7d22f83bfa3a6c80f6.62fbd66ec15bdf6e5322f44f1546f0d475cf07a90caca0912ead31408a83a319
All model checkpoint weights were used when initializing T5ForConditionalGeneration.

All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  8.96ba/s]100%|██████████| 1/1 [00:00<00:00,  8.95ba/s]
Using amp fp16 backend
***** Running training *****
  Num examples = 500
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 125
  0%|          | 0/125 [00:00<?, ?it/s]  1%|          | 1/125 [00:00<01:09,  1.79it/s]  2%|▏         | 2/125 [00:00<00:53,  2.29it/s]  2%|▏         | 3/125 [00:00<00:42,  2.86it/s]  3%|▎         | 4/125 [00:01<00:34,  3.46it/s]  4%|▍         | 5/125 [00:01<00:29,  4.02it/s]  5%|▍         | 6/125 [00:01<00:27,  4.35it/s]  6%|▌         | 7/125 [00:01<00:24,  4.74it/s]  6%|▋         | 8/125 [00:01<00:22,  5.11it/s]  7%|▋         | 9/125 [00:01<00:21,  5.48it/s]  8%|▊         | 10/125 [00:01<00:20,  5.72it/s]  9%|▉         | 11/125 [00:02<00:19,  5.70it/s] 10%|▉         | 12/125 [00:02<00:18,  5.96it/s] 10%|█         | 13/125 [00:02<00:18,  6.15it/s] 11%|█         | 14/125 [00:02<00:17,  6.19it/s] 12%|█▏        | 15/125 [00:02<00:17,  6.20it/s] 13%|█▎        | 16/125 [00:02<00:17,  6.16it/s] 14%|█▎        | 17/125 [00:03<00:17,  6.22it/s] 14%|█▍        | 18/125 [00:03<00:17,  6.00it/s] 15%|█▌        | 19/125 [00:03<00:17,  6.05it/s] 16%|█▌        | 20/125 [00:03<00:16,  6.18it/s] 17%|█▋        | 21/125 [00:03<00:16,  6.24it/s] 18%|█▊        | 22/125 [00:03<00:16,  6.32it/s] 18%|█▊        | 23/125 [00:04<00:16,  6.25it/s] 19%|█▉        | 24/125 [00:04<00:16,  6.25it/s] 20%|██        | 25/125 [00:04<00:15,  6.36it/s] 21%|██        | 26/125 [00:04<00:15,  6.42it/s] 22%|██▏       | 27/125 [00:04<00:15,  6.36it/s] 22%|██▏       | 28/125 [00:04<00:15,  6.41it/s] 23%|██▎       | 29/125 [00:05<00:15,  6.18it/s] 24%|██▍       | 30/125 [00:05<00:15,  6.02it/s] 25%|██▍       | 31/125 [00:05<00:15,  6.14it/s] 26%|██▌       | 32/125 [00:05<00:14,  6.22it/s] 26%|██▋       | 33/125 [00:05<00:14,  6.21it/s] 27%|██▋       | 34/125 [00:05<00:14,  6.33it/s] 28%|██▊       | 35/125 [00:05<00:13,  6.49it/s] 29%|██▉       | 36/125 [00:06<00:13,  6.58it/s] 30%|██▉       | 37/125 [00:06<00:13,  6.71it/s] 30%|███       | 38/125 [00:06<00:13,  6.56it/s] 31%|███       | 39/125 [00:06<00:13,  6.46it/s] 32%|███▏      | 40/125 [00:06<00:12,  6.54it/s] 33%|███▎      | 41/125 [00:06<00:12,  6.51it/s] 34%|███▎      | 42/125 [00:07<00:12,  6.54it/s] 34%|███▍      | 43/125 [00:07<00:12,  6.45it/s] 35%|███▌      | 44/125 [00:07<00:12,  6.45it/s] 36%|███▌      | 45/125 [00:07<00:12,  6.47it/s] 37%|███▋      | 46/125 [00:07<00:12,  6.44it/s] 38%|███▊      | 47/125 [00:07<00:12,  6.49it/s] 38%|███▊      | 48/125 [00:07<00:12,  6.28it/s] 39%|███▉      | 49/125 [00:08<00:11,  6.41it/s] 40%|████      | 50/125 [00:08<00:11,  6.39it/s] 41%|████      | 51/125 [00:08<00:11,  6.36it/s] 42%|████▏     | 52/125 [00:08<00:11,  6.38it/s] 42%|████▏     | 53/125 [00:08<00:11,  6.39it/s] 43%|████▎     | 54/125 [00:08<00:11,  6.41it/s] 44%|████▍     | 55/125 [00:09<00:10,  6.40it/s] 45%|████▍     | 56/125 [00:09<00:10,  6.28it/s] 46%|████▌     | 57/125 [00:09<00:10,  6.26it/s] 46%|████▋     | 58/125 [00:09<00:11,  6.00it/s] 47%|████▋     | 59/125 [00:09<00:10,  6.08it/s] 48%|████▊     | 60/125 [00:09<00:10,  6.11it/s] 49%|████▉     | 61/125 [00:10<00:10,  6.21it/s] 50%|████▉     | 62/125 [00:10<00:09,  6.32it/s] 50%|█████     | 63/125 [00:10<00:09,  6.31it/s] 51%|█████     | 64/125 [00:10<00:09,  6.27it/s] 52%|█████▏    | 65/125 [00:10<00:09,  6.08it/s] 53%|█████▎    | 66/125 [00:10<00:09,  6.21it/s] 54%|█████▎    | 67/125 [00:11<00:09,  6.25it/s] 54%|█████▍    | 68/125 [00:11<00:09,  6.22it/s] 55%|█████▌    | 69/125 [00:11<00:09,  6.18it/s] 56%|█████▌    | 70/125 [00:11<00:08,  6.30it/s] 57%|█████▋    | 71/125 [00:11<00:08,  6.36it/s] 58%|█████▊    | 72/125 [00:11<00:08,  6.51it/s] 58%|█████▊    | 73/125 [00:11<00:07,  6.53it/s] 59%|█████▉    | 74/125 [00:12<00:07,  6.59it/s] 60%|██████    | 75/125 [00:12<00:07,  6.39it/s] 61%|██████    | 76/125 [00:12<00:07,  6.39it/s] 62%|██████▏   | 77/125 [00:12<00:07,  6.48it/s] 62%|██████▏   | 78/125 [00:12<00:07,  6.52it/s] 63%|██████▎   | 79/125 [00:12<00:07,  6.49it/s] 64%|██████▍   | 80/125 [00:13<00:07,  6.35it/s] 65%|██████▍   | 81/125 [00:13<00:06,  6.40it/s] 66%|██████▌   | 82/125 [00:13<00:06,  6.41it/s] 66%|██████▋   | 83/125 [00:13<00:06,  6.31it/s] 67%|██████▋   | 84/125 [00:13<00:06,  6.40it/s] 68%|██████▊   | 85/125 [00:13<00:06,  6.31it/s] 69%|██████▉   | 86/125 [00:13<00:06,  6.49it/s] 70%|██████▉   | 87/125 [00:14<00:05,  6.66it/s] 70%|███████   | 88/125 [00:14<00:05,  6.47it/s] 71%|███████   | 89/125 [00:14<00:05,  6.34it/s] 72%|███████▏  | 90/125 [00:14<00:05,  6.47it/s] 73%|███████▎  | 91/125 [00:14<00:05,  6.50it/s] 74%|███████▎  | 92/125 [00:14<00:05,  6.50it/s] 74%|███████▍  | 93/125 [00:15<00:04,  6.46it/s] 75%|███████▌  | 94/125 [00:15<00:04,  6.51it/s] 76%|███████▌  | 95/125 [00:15<00:04,  6.45it/s] 77%|███████▋  | 96/125 [00:15<00:04,  6.51it/s] 78%|███████▊  | 97/125 [00:15<00:04,  6.52it/s] 78%|███████▊  | 98/125 [00:15<00:04,  6.64it/s] 79%|███████▉  | 99/125 [00:15<00:03,  6.53it/s] 80%|████████  | 100/125 [00:16<00:03,  6.33it/s] 81%|████████  | 101/125 [00:16<00:03,  6.46it/s] 82%|████████▏ | 102/125 [00:16<00:03,  6.50it/s] 82%|████████▏ | 103/125 [00:16<00:03,  6.48it/s] 83%|████████▎ | 104/125 [00:16<00:03,  6.49it/s] 84%|████████▍ | 105/125 [00:16<00:03,  6.46it/s] 85%|████████▍ | 106/125 [00:17<00:02,  6.38it/s] 86%|████████▌ | 107/125 [00:17<00:02,  6.47it/s] 86%|████████▋ | 108/125 [00:17<00:02,  6.35it/s] 87%|████████▋ | 109/125 [00:17<00:02,  6.23it/s] 88%|████████▊ | 110/125 [00:17<00:02,  6.44it/s] 89%|████████▉ | 111/125 [00:17<00:02,  6.38it/s] 90%|████████▉ | 112/125 [00:18<00:02,  6.37it/s] 90%|█████████ | 113/125 [00:18<00:01,  6.30it/s] 91%|█████████ | 114/125 [00:18<00:01,  6.25it/s] 92%|█████████▏| 115/125 [00:18<00:01,  6.38it/s] 93%|█████████▎| 116/125 [00:18<00:01,  6.41it/s] 94%|█████████▎| 117/125 [00:18<00:01,  6.36it/s] 94%|█████████▍| 118/125 [00:18<00:01,  6.28it/s] 95%|█████████▌| 119/125 [00:19<00:00,  6.01it/s] 96%|█████████▌| 120/125 [00:19<00:00,  6.23it/s] 97%|█████████▋| 121/125 [00:19<00:00,  6.31it/s] 98%|█████████▊| 122/125 [00:19<00:00,  6.49it/s] 98%|█████████▊| 123/125 [00:19<00:00,  6.53it/s] 99%|█████████▉| 124/125 [00:19<00:00,  6.54it/s]100%|██████████| 125/125 [00:20<00:00,  6.46it/s]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 125/125 [00:20<00:00,  6.46it/s]100%|██████████| 125/125 [00:20<00:00,  6.24it/s]
Saving model checkpoint to output_dir
Configuration saved in output_dir/config.json
Model weights saved in output_dir/pytorch_model.bin
tokenizer config file saved in output_dir/tokenizer_config.json
Special tokens file saved in output_dir/special_tokens_map.json
Copy vocab file to output_dir/spiece.model
***** train metrics *****
  epoch                      =        1.0
  init_mem_cpu_alloc_delta   =     -565MB
  init_mem_cpu_peaked_delta  =     2808MB
  init_mem_gpu_alloc_delta   =     2814MB
  init_mem_gpu_peaked_delta  =        0MB
  train_mem_cpu_alloc_delta  =      783MB
  train_mem_cpu_peaked_delta =        0MB
  train_mem_gpu_alloc_delta  =     2814MB
  train_mem_gpu_peaked_delta =     2301MB
  train_runtime              = 0:00:20.06
  train_samples              =        500
  train_samples_per_second   =      6.229
