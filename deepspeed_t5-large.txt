WARNING:__main__:You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `
WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='output_dir', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May09_19-45-34_fed50493e286', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output_dir', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed='transformers/tests/deepspeed/ds_config_zero2.json', label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=False)
WARNING:datasets.builder:Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)
loading configuration file https://huggingface.co/t5-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1adb1f57c3579debfcce7b94ee03f6144e0ff7a0c2825e48b3f9cde9ce290c7d.35a5d3297357a9ea0fccdf170df8d287f1cad2ee810bca042f98c531c0cab2c6
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 4096,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.6.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading configuration file https://huggingface.co/t5-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1adb1f57c3579debfcce7b94ee03f6144e0ff7a0c2825e48b3f9cde9ce290c7d.35a5d3297357a9ea0fccdf170df8d287f1cad2ee810bca042f98c531c0cab2c6
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 4096,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.6.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file https://huggingface.co/t5-large/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/71ee551f54e246045a7b94dd449c33759924b864712e6d235bbba5245c9f6296.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d
loading file https://huggingface.co/t5-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/276094e085ecb12227136f2e755dc1f68be6f5da32df55ebfb104c791fbbc3c1.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529
loading file https://huggingface.co/t5-large/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/t5-large/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/t5-large/resolve/main/tokenizer_config.json from cache at None
loading weights file https://huggingface.co/t5-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/750feca8cedcd171eb121bd47c3ae16924a473d89f334c7d22f83bfa3a6c80f6.62fbd66ec15bdf6e5322f44f1546f0d475cf07a90caca0912ead31408a83a319
All model checkpoint weights were used when initializing T5ForConditionalGeneration.

All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 10.15ba/s]
Using amp fp16 backend
***** Running training *****
  Num examples = 500
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 63
  0%|          | 0/63 [00:00<?, ?it/s]  2%|▏         | 1/63 [00:00<00:26,  2.30it/s]  3%|▎         | 2/63 [00:00<00:25,  2.39it/s]  5%|▍         | 3/63 [00:01<00:24,  2.44it/s]  6%|▋         | 4/63 [00:01<00:24,  2.45it/s]  8%|▊         | 5/63 [00:03<00:42,  1.37it/s] 10%|▉         | 6/63 [00:04<00:54,  1.04it/s] 11%|█         | 7/63 [00:06<01:02,  1.11s/it] 13%|█▎        | 8/63 [00:07<01:07,  1.23s/it] 14%|█▍        | 9/63 [00:09<01:11,  1.32s/it] 16%|█▌        | 10/63 [00:10<01:13,  1.38s/it] 17%|█▋        | 11/63 [00:11<00:56,  1.09s/it] 19%|█▉        | 12/63 [00:12<01:01,  1.21s/it] 21%|██        | 13/63 [00:14<01:05,  1.31s/it] 22%|██▏       | 14/63 [00:15<01:07,  1.37s/it] 24%|██▍       | 15/63 [00:17<01:07,  1.41s/it] 25%|██▌       | 16/63 [00:18<01:07,  1.43s/it] 27%|██▋       | 17/63 [00:20<01:06,  1.45s/it] 29%|██▊       | 18/63 [00:21<01:05,  1.46s/it] 30%|███       | 19/63 [00:23<01:04,  1.47s/it] 32%|███▏      | 20/63 [00:24<01:02,  1.46s/it] 33%|███▎      | 21/63 [00:25<01:01,  1.47s/it] 35%|███▍      | 22/63 [00:26<00:47,  1.15s/it] 37%|███▋      | 23/63 [00:27<00:50,  1.26s/it] 38%|███▊      | 24/63 [00:29<00:52,  1.35s/it] 40%|███▉      | 25/63 [00:30<00:52,  1.38s/it] 41%|████▏     | 26/63 [00:32<00:52,  1.41s/it] 43%|████▎     | 27/63 [00:33<00:52,  1.45s/it] 44%|████▍     | 28/63 [00:35<00:51,  1.46s/it] 46%|████▌     | 29/63 [00:36<00:49,  1.46s/it] 48%|████▊     | 30/63 [00:38<00:48,  1.47s/it] 49%|████▉     | 31/63 [00:39<00:47,  1.48s/it] 51%|█████     | 32/63 [00:41<00:46,  1.49s/it] 52%|█████▏    | 33/63 [00:42<00:44,  1.49s/it] 54%|█████▍    | 34/63 [00:44<00:43,  1.49s/it] 56%|█████▌    | 35/63 [00:45<00:41,  1.47s/it] 57%|█████▋    | 36/63 [00:47<00:40,  1.49s/it] 59%|█████▊    | 37/63 [00:48<00:38,  1.49s/it] 60%|██████    | 38/63 [00:50<00:37,  1.51s/it] 62%|██████▏   | 39/63 [00:51<00:36,  1.51s/it] 63%|██████▎   | 40/63 [00:53<00:34,  1.50s/it] 65%|██████▌   | 41/63 [00:54<00:32,  1.50s/it] 67%|██████▋   | 42/63 [00:56<00:31,  1.50s/it] 68%|██████▊   | 43/63 [00:57<00:30,  1.51s/it] 70%|██████▉   | 44/63 [00:59<00:28,  1.50s/it] 71%|███████▏  | 45/63 [01:00<00:27,  1.50s/it] 73%|███████▎  | 46/63 [01:02<00:25,  1.51s/it] 75%|███████▍  | 47/63 [01:03<00:24,  1.51s/it] 76%|███████▌  | 48/63 [01:05<00:22,  1.49s/it] 78%|███████▊  | 49/63 [01:06<00:20,  1.50s/it] 79%|███████▉  | 50/63 [01:08<00:19,  1.49s/it] 81%|████████  | 51/63 [01:09<00:18,  1.50s/it] 83%|████████▎ | 52/63 [01:11<00:16,  1.50s/it] 84%|████████▍ | 53/63 [01:12<00:14,  1.50s/it] 86%|████████▌ | 54/63 [01:14<00:13,  1.49s/it] 87%|████████▋ | 55/63 [01:15<00:11,  1.50s/it] 89%|████████▉ | 56/63 [01:17<00:10,  1.50s/it] 90%|█████████ | 57/63 [01:18<00:08,  1.49s/it] 92%|█████████▏| 58/63 [01:20<00:07,  1.49s/it] 94%|█████████▎| 59/63 [01:21<00:05,  1.49s/it] 95%|█████████▌| 60/63 [01:23<00:04,  1.49s/it] 97%|█████████▋| 61/63 [01:24<00:02,  1.49s/it] 98%|█████████▊| 62/63 [01:26<00:01,  1.48s/it]100%|██████████| 63/63 [01:27<00:00,  1.49s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 63/63 [01:27<00:00,  1.49s/it]100%|██████████| 63/63 [01:27<00:00,  1.39s/it]
Saving model checkpoint to output_dir
Configuration saved in output_dir/config.json
Model weights saved in output_dir/pytorch_model.bin
tokenizer config file saved in output_dir/tokenizer_config.json
Special tokens file saved in output_dir/special_tokens_map.json
Copy vocab file to output_dir/spiece.model
***** train metrics *****
  epoch                      =        1.0
  init_mem_cpu_alloc_delta   =        0MB
  init_mem_cpu_peaked_delta  =        0MB
  init_mem_gpu_alloc_delta   =        0MB
  init_mem_gpu_peaked_delta  =        0MB
  train_mem_cpu_alloc_delta  =    14352MB
  train_mem_cpu_peaked_delta =     2791MB
  train_mem_gpu_alloc_delta  =     1469MB
  train_mem_gpu_peaked_delta =     4940MB
  train_runtime              = 0:01:27.78
  train_samples              =        500
  train_samples_per_second   =      0.718
